//(showing answer)gpt한테 물어봄.

#include <iostream>
#include <algorithm>
using namespace std;

int n;
int a[1001], d[1001];
//1. 테이블정의
//d[i]=i번째 인덱스까지, 가능한 가장 긴 증가수열의 길이

int main(){
    ios::sync_with_stdio(0);
    cin.tie(0);
    cin>>n;
    for(int i=0; i<n; i++){
        cin>>a[i];
    }
    fill(d, d + n, 1); // d 배열을 모두 1로 초기화
    //위의 이거 했더니 통과함. -> !!!!!!!오류!!!!!! 주석 참고
    //d[0]=1; //3. 초기값 대입
    for(int i=1; i<n; i++){
        for(int j=0; j<i; j++){ //실수!, n이 아니라 i임. (j<n아님, j<i임)
            //2.점화식 찾기
            if(a[j]<a[i]) d[i]=max(d[i], d[j]+1);
        }
    }
    cout<<*max_element(d, d+n);
}

/*
이전문제인
boj11055(가장큰 증가하는 부분수열)의
o(n^2)방식을 적용하였음.

단지 초기값을 d[0]=1, 나머지는 d의 인자들은 0으로
max함수 안 연산에서 a[i]가 아닌 1을 더해줬을 뿐임.
나머지는 거진동일
*/

/*
!!!!!!!!!!!!!!!!!!!!!!!!!!오류!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
>>>>  d배열을 fill(d, d+n, 1)을 통해 1로 초기화 해줘야함.  <<<
d[1]=0만 한경우

반례
3
10 6 7
일 때
d[]배열이
1 0 1과 같이 나옴.

왜냐?
i=1이고 j=0일때, (a[i]<a[j])규칙에 따라 a[0]<a[1] = 10<6 => false이므로 
d[1]이 0으로 남음.(원래는 테이블 정의에 의해 1이어야함)

이후 i=2에서, j=1일때
d[1]+1을 하면 원래는 1이어햘 d[1]이 0이어서 d[1]+1의 값이 원랜 2어야하는데 1이됨.

즉, d배열의 값들이 제대로 1로 초기화되지 않는 경우 d[i]의 값이잘못 계산될 수 있음.

따라서 모든 d배열을 1로 초기화해야함.(d테이블 정의에 의해 모든 d[i]는 최소 1이어야함)

=> 실수한 이유. 
d[0]=1로하면
3
10 10 10
인 경우 
max_element에서
최소한 1이 도출되니까.
문제없을 거라 판단했음.
게다가 맨처음 d[0]=1만하면 나머지는 알아서 계산될거라 생각했음.
if를 고려하지 못한 실수.

==>교훈.
다음에는 괜히 맨처음꺼만 초기화하는 객기부리지 말고,
그냥 안전하게 테이블 정의에 맞게
n범위안 모든 배열을 초기화 할것.

*/